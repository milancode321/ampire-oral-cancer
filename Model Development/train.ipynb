{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e6c0c76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pycocotools in ./.venv/lib/python3.8/site-packages (2.0.7)\n",
      "Requirement already satisfied: matplotlib in ./.venv/lib/python3.8/site-packages (3.7.5)\n",
      "Requirement already satisfied: opencv-python in ./.venv/lib/python3.8/site-packages (4.11.0.86)\n",
      "Requirement already satisfied: torch in ./.venv/lib/python3.8/site-packages (2.2.2)\n",
      "Requirement already satisfied: torchvision in ./.venv/lib/python3.8/site-packages (0.17.2)\n",
      "Requirement already satisfied: numpy in ./.venv/lib/python3.8/site-packages (from pycocotools) (1.24.4)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in ./.venv/lib/python3.8/site-packages (from matplotlib) (1.1.1)\n",
      "Requirement already satisfied: cycler>=0.10 in ./.venv/lib/python3.8/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in ./.venv/lib/python3.8/site-packages (from matplotlib) (4.57.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in ./.venv/lib/python3.8/site-packages (from matplotlib) (1.4.7)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.8/site-packages (from matplotlib) (24.2)\n",
      "Requirement already satisfied: pillow>=6.2.0 in ./.venv/lib/python3.8/site-packages (from matplotlib) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in ./.venv/lib/python3.8/site-packages (from matplotlib) (3.1.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in ./.venv/lib/python3.8/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in ./.venv/lib/python3.8/site-packages (from matplotlib) (6.4.5)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.8/site-packages (from torch) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in ./.venv/lib/python3.8/site-packages (from torch) (4.13.2)\n",
      "Requirement already satisfied: sympy in ./.venv/lib/python3.8/site-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: networkx in ./.venv/lib/python3.8/site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.8/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in ./.venv/lib/python3.8/site-packages (from torch) (2025.3.0)\n",
      "Requirement already satisfied: zipp>=3.1.0 in ./.venv/lib/python3.8/site-packages (from importlib-resources>=3.2.0->matplotlib) (3.20.2)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.8/site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.8/site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.venv/lib/python3.8/site-packages (from sympy->torch) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pycocotools matplotlib opencv-python torch torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "51520dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd06d5ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "from pycocotools.coco import COCO\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7a66d4bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CONFIG ===\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "BASE_PATH = \"/Users/hardikm-visiobyte/Desktop/Oral_Cancer_Ampire/dataset\"\n",
    "BATCH_SIZE = 16\n",
    "NUM_EPOCHS = 5\n",
    "NUM_CLASSES = 3  # Abnormal, Normal, Null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "86be197f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Custom Dataset ===\n",
    "class OralCancerCOCODataset(Dataset):\n",
    "    def __init__(self, folder):\n",
    "        self.img_dir = os.path.join(BASE_PATH, folder)\n",
    "        self.ann_path = os.path.join(self.img_dir, '_annotations.coco.json')\n",
    "        self.coco = COCO(self.ann_path)\n",
    "        self.image_ids = list(self.coco.getImgIds())\n",
    "\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Loop until a valid label is found (excluding category_id = 0)\n",
    "        while True:\n",
    "            img_id = self.image_ids[idx]\n",
    "            img_info = self.coco.loadImgs(img_id)[0]\n",
    "            img_path = os.path.join(self.img_dir, img_info['file_name'])\n",
    "\n",
    "            ann_ids = self.coco.getAnnIds(imgIds=img_id)\n",
    "            anns = self.coco.loadAnns(ann_ids)\n",
    "\n",
    "            if len(anns) == 0:\n",
    "                idx = (idx + 1) % len(self.image_ids)\n",
    "                continue\n",
    "\n",
    "            label = anns[0]['category_id']\n",
    "            if label == 0:\n",
    "                idx = (idx + 1) % len(self.image_ids)\n",
    "                continue  # skip invalid category\n",
    "\n",
    "            image = Image.open(img_path).convert('RGB')\n",
    "            image = self.transform(image)\n",
    "\n",
    "            label = label - 1  # Make Abnormal=0, Normal=1, Null=2\n",
    "            return image, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "44f6005f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "# === Loaders ===\n",
    "train_loader = DataLoader(OralCancerCOCODataset('train'), batch_size=BATCH_SIZE, shuffle=True)\n",
    "valid_loader = DataLoader(OralCancerCOCODataset('valid'), batch_size=BATCH_SIZE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "00b904b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /Users/hardikm-visiobyte/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
      "100.0%\n"
     ]
    }
   ],
   "source": [
    "# === Model ===\n",
    "model = models.resnet18(pretrained=True)\n",
    "model.fc = nn.Linear(model.fc.in_features, NUM_CLASSES)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ac9d5af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Loss & Optimizer ===\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b94a27de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5] Loss: 16.2039 | Accuracy: 28.42%\n",
      "Epoch [2/5] Loss: 3.4937 | Accuracy: 96.17%\n",
      "Epoch [3/5] Loss: 1.1021 | Accuracy: 98.91%\n",
      "Epoch [4/5] Loss: 0.4175 | Accuracy: 100.00%\n",
      "Epoch [5/5] Loss: 0.3242 | Accuracy: 100.00%\n"
     ]
    }
   ],
   "source": [
    "# === Training Loop ===\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    acc = 100 * correct / total\n",
    "    print(f\"Epoch [{epoch + 1}/{NUM_EPOCHS}] Loss: {total_loss:.4f} | Accuracy: {acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a8a7265d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model saved as 'oral_cancer_classifier.pth'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# === Save the model ===\n",
    "torch.save(model.state_dict(), \"oral_cancer_classifier.pth\")\n",
    "print(\"✅ Model saved as 'oral_cancer_classifier.pth'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a8f4267",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
